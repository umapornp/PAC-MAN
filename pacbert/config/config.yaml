# --------------------------
# CONFIGURATION FOR PACBERT.
# --------------------------

# Device
DEVICE: "auto"    # Device. {'cpu', 'cuda', 'auto'}

# Data
BATCH_SIZE: 128   # Batch size.
NUM_WORKERS: 2    # Number of workers for dataloader.

# Train
DO_TRAIN: True    # Whether to train the model.
DO_TEST: True     # Whether to evaluate the model.
BEGIN_EPOCH: 1    # Begin epoch (start from 1). For resume mode, the model will resume from the checkpoint epoch.
END_EPOCH: 4      # End epoch.

# Acceleration
USE_DDP: True     # Whether to use DDP.
USE_AMP: True     # Whether to use AMP (Available for GPU only).
DTYPE: bfloat16   # Data type. {'float16', 'bfloat16', 'float32'} (bfloat16 for cpu)

# Initiation
INIT_TYPE: ""     # Initial type. {'checkpoint', 'pretrained', ''} Leave blank for scratch.
INIT_FROM: ""     # Initial from. {'./pacbert/outputs/ckpt.pt', 'bert-base-uncased', ''} Leave blank for scratch.
GNN_PATH: ""      # Path of pretrained MANGNN.    {'./mangnn/outputs/ckpt.pt', ''} Leave blank for scratch.
GNN_CONFIG: ""    # Path of MANGNN configuration. {'./mangnn/config/config.yaml', ''} Leave blank for scratch.

# Checkpoint
SAVE_EVERY: 2                             # Interval steps for model checkpoint.
CKPT_PATH: "./pacbert/outputs"            # Checkpoint path.
TSB_DIR: "./pacbert/outputs/logs"         # Tensorboard path.
JSON_DIR: "./pacbert/outputs/result.json" # Result path.

# Optimizer and learning rate
OPTIMIZER: "AdamW"
WEIGHT_DECAY: 1e-2
BETAS: (0.9, 0.95)
LR: 1e-4
LR_FACTOR: 0.1
LR_SCHEDULER: "warmup_linear"   # LR scheduler. {'plateau', 'warmup_linear', 'warmup_cosine'}
WARMUP_STEPS: 500               # Warmup steps for LR scheduler. 0 for no warmup.

# Gradient
GRADIENT_ACCUMULATION_STEPS: 1  # 1 is disable
CLIP_GRAD: 1.0                  # 0.0 is disable

# Metrics
TOPK: [1,3,5,7,9]

# PACBert model
PACBERT:
  user_size: 6387
  tag_size: 3150
  vocab_size: 30522
  hidden_size: 768
  gnn_hidden_size: 64
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  hidden_act: "gelu"
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 512
  type_vocab_size: 3
  initializer_range: 0.02
  layer_norm_eps: 1e-12
  pad_token_id: 0
  position_embedding_type: "absolute"
  use_cache: True
  classifier_dropout:

TRAIN_DATA_PATH: "pacbert/data/twitter/twitter_train.json"
VAL_DATA_PATH: "pacbert/data/twitter/twitter_val.json"
TEST_DATA_PATH: "pacbert/data/twitter/twitter_test.json"
TAG_PATH: "pacbert/data/twitter/tag.txt"
